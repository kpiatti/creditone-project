{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify data directory\n",
    "root = Path('creditone-project\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d70b72287d3b>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(root , 'cleaned_transformed_credit_one_data.csv')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'creditone-project\\\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d70b72287d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import cleaned data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'cleaned_transformed_credit_one_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\course1-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\course1-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\course1-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\course1-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1187\u001b[0m                     \u001b[1;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m                 )\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\course1-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   2380\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comment_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2382\u001b[1;33m         f, handles = get_handle(\n\u001b[0m\u001b[0;32m   2383\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\course1-env\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'creditone-project\\\\data'"
     ]
    }
   ],
   "source": [
    "#import cleaned data \n",
    "df = pd.read_csv(root , 'cleaned_transformed_credit_one_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify imported data looks as expected\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Info\n",
    "* Location: raw data is stored in the MySQL database.\n",
    "* After a SQL query, I downloaded the raw data .csv file, cleaned it in a jupyter notebook (c2t1-kp-creditone-processing.ipynb), and exprted the cleaned data as .csv file ('cleaned_transformed_credit_one_data.csv'). \n",
    "* I imported the cleaned data file into this eda notebook for initial exploration.\n",
    "\n",
    "* Cleaned data characteristics: 30,000 obervations & 25 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get variable data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables by Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Demographic Variables\n",
    "    * Sex—categorical, 2 levels: male, female\n",
    "    * Education— ordinal, 6 levels? \n",
    "    * Marital Status - categorical, 4 levels\n",
    "    * Age- continuous\n",
    "    \n",
    "2. Account Variables\n",
    "    * ID- customer ID- unique number assigned to each observation in dataset (exclude from analysis)\n",
    "    * Balance Limit - continuous\n",
    "    * Default Status - categorical 2 level   \n",
    "    * Repayment Status in April  - categorical, 9+ levels\n",
    "    * Repayment Status in May \n",
    "    * Repayment Status in June\n",
    "    * Repayment Status in July\n",
    "    * Repayment Status in August\n",
    "    * Repayment Status in September\n",
    "    \n",
    "3. Billing Variables\n",
    "    * April bill amount - continuous\n",
    "    * May bill amount\n",
    "    * June bill amount\n",
    "    * July Bill amount\n",
    "    * August bill amount\n",
    "    * September bill amount\n",
    "    \n",
    "4. Payment Variables\n",
    "    * April Payment amount- continuous\n",
    "    * May Payment Amount\n",
    "    * June Payment Amount \n",
    "    * July Payment Amount\n",
    "    * August Payment Amount\n",
    "    * September Payment Amount\n",
    "    \n",
    "* Target Features\n",
    "    * Balance Limit- if we are trying to predict how much credit customers should be extended. \n",
    "    * Default- if we are trying to predict which customers are most likely to dafault on their loans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univarite Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create in-line minimal pandas profile report\n",
    "ProfileReport(df, minimal=True)\n",
    "\n",
    "#profile = ProfileReport(df, title='Clean CreditOne Pandas Profiling Report', explorative = True)\n",
    "#profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* The full pandas profile report took a *very long time to generate* and was *too big* for my browser, processor, and/or memory to fully render. \n",
    ">* Two options to generate profile reports more quickly:\n",
    "    * Generate a minimal report (which does not include correlations between vars) by adding a 'minimal=True' parameter to ProfileReport command.\n",
    "    * Generate a report for only a *sample* of df by substituting ```df.sample(n=10000)``` for df in the command. \n",
    ">* To output the report as a separate html file use:  \n",
    ">```prof = ProfileReport(df)\n",
    "prof.to_file(output_file='output.html')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations (based on profile report)*\n",
    "* Sex \n",
    "    * 60.4% female, 39.6% male\n",
    "* Education \n",
    "    * HS 16.4%, University 46.8%, Graduate School 35.3%, Other 1.6%\n",
    "    * Compared to US population, sample has a lot of customers with university and graduate school degrees, and a small number with high school degrees.\n",
    "    * May consider combining university and graduate school categories later.\n",
    "* Marital Status\n",
    "    * Married (1) 45.5%, Single (2) 53.6%, Divorced (3) 1.1%, Other 0.2%\n",
    "* Age\n",
    "    * Range 21 - 79.\n",
    "    * Mean 35, Median 34\n",
    "    * Consider binnning: young, middle age, 65+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations (based on profile report)*\n",
    "\n",
    "* ***Credit/Balance Limit*** (target for model predicting amount of credit to give customers)\n",
    "  * Continuous variable, range - 10k to 1 million\n",
    "  * Only 81 distinct values (consider 8 bins) \n",
    "  * Mean: 167,484\n",
    "  * Distribution significantly skewed toward low end\n",
    "  * Mode: 50k limit (11.2%)\n",
    "  * Only 1 customer with 1 million limit  \n",
    "  * Potential problem: this variable represents the amount of credit given to the customer, including both the individual consumer credit and \"his/her family (supplemental) credit\". I'm not exactly sure what family credit is, but it may be an amount of credit extended to a family group that the customer can use because he/she is part of the family. This potentially muddies the waters because  we don't know how much of the credit limit was given to the customer as an individual vs. how much of the credit limit they were given as a member of a family. At the very least this means, at best the model will be able to accurately predict the amount of individual and family credit each customer should be given. \n",
    "  \n",
    "    \n",
    "* ***Default*** (target for model predicting which customers are likely to default)\n",
    "  * Not defult 77.9%, default 22.1%\n",
    "  * Do I need to use special techniques |when trying to model/predict extremely unlikely events?\n",
    "\n",
    "\n",
    "* Repayment Variables\n",
    "  * -2 = no use, -1 = paid in full, 0 = use revolving credit, 1 = payment delay 1 month, 2 = payment delay 2 months, ..., 9 = payment delay 9+ months\n",
    "  * All repayment variables have roughly same distribution and characteristics: over 50% zeros, significantly more -2 & -1 values than values greater than or equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Billing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations (from profile report)*  \n",
    "* All billing variables have negative minimum values. If these are valid data points, it suggests CreditOne is regularly mistakenly overcharging customers and having to credit them money.\n",
    "* Distributions for all billing variables look roughly the same.\n",
    "    * Very significant left skew. \n",
    "    * Lots of zeros.\n",
    "    * All months contain a small number of negative values. Don't know how to interpret. \n",
    "        * ◘ Consider removing or transforming negative values to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out the rows where the value for at least one of the billing vars is less than zero at least one of the bill\n",
    "df.query('bill_ap < 0 or bill_m < 0 or bill_ju < 0 or bill_jy < 0 or bill_ag or bill_s < 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* I first tried doing the filter using the ``.loc()``` function, but all my attempts threw errors. \n",
    ">  * ```df.loc[ df[['bill_ap', 'bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s']]<0]```Threw a ValueError: cannot index with multidimensional key.\n",
    ">  * ```df[['bill_ap', 'bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s']].loc[lambda x: x<0]``` same error as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out the rows where the value for all the billing vars is less than zero \n",
    "df.query('bill_ap < 0 & bill_m < 0 & bill_ju < 0 & bill_jy < 0 & bill_ag & bill_s < 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort df by april bill amount from highest to lowest, then show first 10 rows\n",
    "df.sort_values(by=['bill_ap'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* Suspicious data point:some customers have bill amounts that exceed their balance limits. Does that mean credit one is not enforcing those limits?\n",
    "    * Maybe I should engineer a feature that is true iff the the customer's bill amount is ever bigger than their balance limit. \n",
    "    ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate median values for all billing variables\n",
    "df[ ['bill_ap', 'bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean absolute deviation (MAD) for all billing vars\n",
    "df[ ['bill_ap', 'bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s']].mad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate descriptive stats for all billing variables\n",
    "df[ ['bill_ap', 'bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* The median, mad, and describe stats are included in profile report. I was just practicing and testing if I could generate stats on a *list of vars* (rather than one at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 6 continuous payment variables - each represents the amount the customer paid towards their bill for the months of April to September 2005.\n",
    "\n",
    "*Observations (based on profile report)*\n",
    "* The distributions of the payment and billing variables have similar shapes. \n",
    "* Notable Differences: \n",
    "  * The payment variables contain a higher percentage of zero values. \n",
    "  * The number and spread of positive values when it comes to the payment variables (which makes sense if customers are using revolving credit and/or not paying their bill.\n",
    "  * The payment variables don't contain any negative values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships Between Features (IVs)  \n",
    "In this section, I will examine whether any of the features (independent variables) are related to one another in a significant way. If any of the features  are highly correlated, that may be because they are actually tracking/representing the same information. Since high corfelation between depedent variables can harm ML models, it needs to be handled.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are billing variables correlated with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate kendall's tau correlation between billing vars \n",
    "df[ ['bill_ap', 'bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s']].corr(method='kendall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* By default corr function calculates pearson's correlation co-efficient.\n",
    ">* But it's only appropriate to use Pearson's if the data is normally distributed and we know from the profile report that the distributions for the billing variables are highly skewed. \n",
    ">* I think there's a way to output only the values below the diagonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name a sample of 1,000 data points selected from the df\n",
    "smpl=df.sample(n=1000) \n",
    "\n",
    "#create pairwise plots of bill features using smpl\n",
    "sns.pairplot(smpl, hue='DEFAULT', kind='scatter', vars=['bill_ap','bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s'])\n",
    "\n",
    "#save plot as png\n",
    "plt.savefig('creditone_bill_pairplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* It's reasonable to suspect the billing variables might be correlated, since it's not uncommon for individuals' spending to be similar from month to month.\n",
    "* Yes, the billing variables are highly correlated with each other. Max: 0.8112, Min: 0.5853.\n",
    "* In general, the correlation is highest between each monthly billing variable and the variable representing the month after. The correlation declines with each proceeding month (e.g. the correlation is highest between the April (bill_ap) and May (bill_m) billing variables, and declines for each subsequent month.)  \n",
    "    * ◘ May impact model performance. Consider only using one of the billing variables in model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the payment variables correlated with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at kendall's tau correlations b/t payment variables\n",
    "df[['pmt_ap', 'pmt_m', 'pmt_ju', 'pmt_jy', 'pmt_ag', 'pmt_s']].corr(method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pairplot of the payment features using smpl (of 1,000)\n",
    "sns.pairplot(smpl, hue='DEFAULT', kind='scatter', vars=['pmt_ap','pmt_m', 'pmt_ju', 'pmt_jy', 'pmt_ag', 'pmt_s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observtions*\n",
    "* I wanted to look at whether the payment variables on related because I thought the amount customers pay each month  might be related to the amount they were billed during the previous month. If that's true and the billing variables are highly correlated, I would expect the payment variables to also be highly correlated. \n",
    "* When we look at the kendall's correlation coefficients between the payment vars they are modest (between 0.348 and 0.4126).\n",
    "* However, when we look at the scatter plots, there doesn't appear to be any obvious relationship. \n",
    "* This suggests, somewhat surprising, that the amount customers pay each month is not really related to the amount they are billed.\n",
    "  * I wonder if the lack of a clear relationship is because I need to look at the relationships between billing vars and payment vars offset by one month (e.g. the relationship between the bill amount for April and the payment amount for May, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the payment variables correlated with the billing variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at kendall's tau correlations b/t payment variables\n",
    "df[['bill_ap', 'bill_m', 'bill_ju', 'bill_jy', 'bill_ag', 'bill_s','pmt_ap', 'pmt_m', 'pmt_ju', 'pmt_jy', 'pmt_ag', 'pmt_s']].corr(method='kendall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* I expected to find a moderate to high correlation between the billing and payment variables, since presumably the amount customers pay each month is at least somewhat related to how much they are billed. \n",
    "* More specifically, we should expect that the payment amount is most highly correlated with the bill amount of the previous month (e.g. the july payment amount is has the strongest correlation with the bill amount for June.\n",
    "* The strongest correlations are around 0.51."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the repayment variables related to the billing or payment variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the repayment history variables should, in some sense, be related the values in the payment variables because the the repayment history variable is representing whether the customer has been using the credit, if they are paying their bills, and if their payments are for the full amount or leave a monthly balance (revolving credit).\n",
    "* However, it's not clear how or what kind of relationship I can look for. One challenge is that the payment variables are continuous, and the repayment variables are categorical. It shouldf be the case that if a customer's monthly payment is not zero (i.e. they paid some amount) for some month, the value for their repayment history variable the next month should be -1 or zero.\n",
    "* I have no idea how I could test for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Relationships Between Features (IVs) & Targets (DVs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Limit Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will perform statistical tests to determine whether any of the DVs are related to the limit_bal target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic Variables & Credit Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=df, annot=True, fmt=\"d\", linewidths=.5)\n",
    "\n",
    "\"\"\"\n",
    "Above code won't work with non-numeric data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sex & Credit Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='SEX',y='LIMIT_BAL' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between the box plots for female and male indicates there is no difference in credit limits between men and women. That means SEX is unlikely to be a helpful variable to include in a model prediting LIMIT_BAL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Education & Credit Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df, kind='box', x='EDUCATION', y='LIMIT_BAL').set_xticklabels(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots for all education levels are very similar. Customers with a graduate education enjoy credit limits that are on average higher than the other groups. It also appears the credit limits for customers with a high school education are slightly skewed to the lower end. The differences aren't dramatic, but a customer's credit limit does appear to be somewhat related to their level of education  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marriage & Credit Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='MARRIAGE', y='LIMIT_BAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are some small differences in the plots, it does not appear that a customer's credit limit is related to their marital status. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age & Credit Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='AGE', y='LIMIT_BAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't appear to be any sort of meaningful relationship between age and credit limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain correlation co-efficients for all features and target\n",
    "df[df.columns[1:]].corr()['DEFAULT'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic Features & Defualt Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sex & Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"SEX\", stat='density', shrink=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=df, x=\"SEX\", col='DEFAULT', stat='density', shrink=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* From the first plot, we can see that CreditOne's customers are slightly more likely to be female than male.\n",
    "* From the second set of plots, when we compare the data for customers who have defualted on their loans to the data for the customers who have not defaulted on their loans, it looks like customers who have defaulted on their loans are slightly more likely to be male than customers overall--however, it's unlikely this difference is statistically significant. \n",
    "* The fact that the relative probabilities of being female vs. male is almost the same among the customers who have defaulted and the customers who have not defualted suggests that sex will not be an important feature in a model predicting whether a customer will defualt on their loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Education & Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create probability chart for the EDU var \n",
    "sns.displot(data=df, x =\"EDUCATION\", discrete=True, shrink = .8, stat='probability').set_xticklabels(rotation=45) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create subplots showing the distribution for the EDU var conditional on the DEFUALT var\n",
    "sns.displot(data=df, x =\"EDUCATION\", col = 'DEFAULT', shrink = .8, stat='probability').set_xticklabels(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* Since higher levels of education are highly correlated with higher levels of income, and higher levels of income are presumably correlated with lower rates of defaulting on loans. I was anticipating that customers with university and graduate school levels of education to be relatively less likely to default on their loans. \n",
    "* However, comparing the distributions of education levels for customers who have defaulted to the ebb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code generates a messed up plot\n",
    "g = sns.FacetGrid(data=df, col=\"DEFAULT\", sharey=False)\n",
    "g.map_dataframe(sns.histplot, x=\"EDUCATION\", stat='probability')\n",
    "g.set_xticklabels(rotation=45) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* I created the g subpolots at fisrt and thought they revealed an interesting relationship between level of edu and whether a customer had defaulted on their loans. It looked like relative to other groups, those with a high school edu level are less likely to default on their loans, and those with the other edu level are more likely. \n",
    ">* However, I continued to mess around and try out different types of plots and use different code, and none of the subsequent plots I generated showed the pattern. They all showed the pattern we would expect to see if there is no significant relationship between level of education and whether a customer defaulted on their loan.\n",
    ">* I investigated the issue extensively on my own and tried to figure it out with David Schwab, but it is still a mystery. \n",
    "  >* David pointed out that in the weird charts it looks like bars for the \"high school\" and \"other\" levels the bars have been swapped. That sounds plausible to me, but we could not figure out how that could have happened, or how to get rid of it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age & Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df, kind='box', x='DEFAULT', y='AGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* We can see in the above boxplot the the mean, IQR (middle 50% of data points), and range for the age variable are nearly identical for customers who defaulted on their loans (blue box) and for customers who did not defualt on their loans (orange box). \n",
    "* This suggests there is no significant relationship between a customer's age and whether or not she defaulted on her loans, which means AGE will not be an important feature in a model predicting whether a customer will default.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Marriage & Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the df to return only rows where value for DEFAULT == default\n",
    "dflt = df.loc[df['DEFAULT']=='default']\n",
    "dflt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the df to return only rows where value for DEFAULT == not default\n",
    "nodflt = df.loc[df['DEFAULT']=='not default']\n",
    "nodflt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(13,5), sharey=True)\n",
    "axes[0].set_title('All Customers', fontsize=14)\n",
    "axes[1].set_title('Customers who defualted', fontsize=14)\n",
    "axes[2].set_title('Customers who did not default', fontsize=14)\n",
    "\n",
    "# To iterate over all items in a multidimensional numpy array, use the `flat` attribute\n",
    "for ax in axes.flat:\n",
    "    # set xticks...\n",
    "    ax.set(xticks=[0, 1, 2, 3])\n",
    "\n",
    "fig.suptitle(t='Is there a relationship between customers marital status & whether they defaulted?', verticalalignment='baseline', fontsize=18)\n",
    "labels=['Other', 'Married', 'Single', 'Divorced']\n",
    "\n",
    "\n",
    "sns.histplot(ax=axes[0], data=df, x ='MARRIAGE', stat='probability', discrete=True, shrink=.8)\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_xticklabels(labels=labels, rotation=45)\n",
    "\n",
    "sns.histplot(ax=axes[1], data=dflt, x ='MARRIAGE', stat='probability', discrete=True, shrink=.8)\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_xticklabels(labels=labels, rotation=45)\n",
    "\n",
    "sns.histplot(ax=axes[2], data=nodflt, x ='MARRIAGE', stat='probability', discrete=True, shrink=.8)\n",
    "axes[2].set_xticklabels(labels=labels, rotation=45)\n",
    "axes[2].set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* It look several hours to get the above row of plots. \n",
    ">* Problems I had to solve:\n",
    "  * Because the number of defualts is much smaller than not defaults, to compare sex across the two categories I couldn't use count as the y axis, I needed to plot a normalized stat like probability or density. \n",
    "  * Using hue: When I tried to use hue to represent the DEFAULT categories and plot them on one set of axes, the resulting graph had 4 bars (2 for female, 2 male) whose probabilities summed to 1. Which meant the bars for male an female in the default category were much smaller than the bars for male and female in the not default category. \n",
    "  * I also couldn\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* From a comparison on the above plots, there appears that single customers are relatively less likely to default on their loans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Billing Variables & Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20,18), sharey=True)\n",
    "\n",
    "fig.suptitle('Monthly Bill Amounts Grouped by DEFAULT')\n",
    "\n",
    "sns.boxplot(ax=axes[0, 0], data=df, x='DEFAULT', y='bill_ap')\n",
    "sns.boxplot(ax=axes[0, 1], data=df, x='DEFAULT', y='bill_m')\n",
    "sns.boxplot(ax=axes[0, 2], data=df, x='DEFAULT', y='bill_ju')\n",
    "sns.boxplot(ax=axes[1, 0], data=df, x='DEFAULT', y='bill_jy')\n",
    "sns.boxplot(ax=axes[1, 1], data=df, x='DEFAULT', y='bill_ag')\n",
    "sns.boxplot(ax=axes[1, 2], data=df, x='DEFAULT', y='bill_s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    "* Before finding the code to create the above 2x6 grid of subplots, I created the boxplots for each billing variable separately. For future refernce here is the code I used:  \n",
    "  * ```sns.catplot(data=df, kind='box', x='DEFAULT', y='bill_ap')```\n",
    "  * ```sns.catplot(data=df, kind='box', x='DEFAULT', y='bill_m', height=6)```\n",
    "  * ```sns.catplot(data=df, kind='box', x='DEFAULT', y='bill_ju', height=6, aspect=.5)```\n",
    "  * ```sns.catplot(data=df, kind='box', x='DEFAULT', y='bill_ag', height=8, aspect=.5)```\n",
    "  * ```sns.catplot(data=df, kind='box', x='DEFAULT', y='bill_s', height=8, aspect=.5)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* I have more questions than observations.\n",
    "  * It looks like there are some significant outliers. How can we find those data points in our df.\n",
    "  * Other than some differences in outliers and a noticable difference in the length of the wiskers in bill_ag, all the box plots are basically the same. What, if anything, can we conclude from that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payment Variables & Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20,25), sharey=True)\n",
    "\n",
    "fig.suptitle('Monthly Payment Variables Grouped by DEFAULT')\n",
    "\n",
    "sns.boxplot(ax=axes[0, 0], data=df, x='DEFAULT', y='pmt_ap')\n",
    "sns.boxplot(ax=axes[0, 1], data=df, x='DEFAULT', y='pmt_m')\n",
    "sns.boxplot(ax=axes[0, 2], data=df, x='DEFAULT', y='pmt_ju')\n",
    "sns.boxplot(ax=axes[1, 0], data=df, x='DEFAULT', y='pmt_jy')\n",
    "sns.boxplot(ax=axes[1, 1], data=df, x='DEFAULT', y='pmt_ag')\n",
    "sns.boxplot(ax=axes[1, 2], data=df, x='DEFAULT', y='pmt_s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "* I assume the boxplots for the payment variables look like this because there is a high % of zeros in these variables. \n",
    "* I also seem some signigicant outliers. \n",
    "\n",
    "\n",
    "* Questions:\n",
    "  * I don't have a good sense of how to interpret/understand what I'm seeing when I compare visualizations of DVs conditional on a target variable. Am I just looking for significant differences? What conclusions would you draw from seeing the above plots for the payment variables? Are there any rules of thumb?\n",
    "  * How do you go about figuring out whether it would improve the situation to bin the values, remove outliers, aggregate the variables, etc? There are too many possibilities to test them all. How do you narrow things down? Is there a way to determine which changes are most likely to have an impact so you can focus your efforts? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Non-numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns with non-numeric data and create a new df with only those columns (so changes aren't in original df)\n",
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data types from object to category to make transforminmg faster\n",
    "cat_df = obj_df[['SEX', 'EDUCATION', 'DEFAULT']].astype('category')\n",
    "\n",
    "\"\"\"\n",
    "I read that it's more efficient to transform non-numerical data into numerical data\n",
    "if the vars to be transformed have the 'category data type instead of 'object' dtype\n",
    "To find out how long things take to run use the %timeit magic\n",
    "\"\"\"\n",
    "cat_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import label encoder from sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#instiate the label encoder function, call it le\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to  encode values in SEX column as numbers and add new column to df to preserve encoded values \n",
    "cat_df['sex_le']=le.fit_transform(cat_df['SEX'])\n",
    "\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to  encode values in SEX column as numbers and add new column to df to preserve encoded values\n",
    "cat_df['dflt_le']=le.fit_transform(cat_df['DEFAULT'])\n",
    "\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(cat_df, columns=['EDUCATION'], prefix=['edu'])\n",
    "\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* The first time I executed the ```get_dummies``` code it didn't do anything. \n",
    ">* I had to add ```cat_df=``` before ```pd.get_dummies``` for the dummy variables to show up in the df.\n",
    ">* I asked why I needed the ```cat_df=``` before get_dummies by not other things (e.g. df.head)? Mike said the reason is that without the assignment (```cat_df=```) at the beginning of the line, pandas doesn't assign the output of the get_dummies operation to anything, and so it isn't preserved. To put it differently, putting ```cat_df=``` at the front of the line does to same thing as adding an ```inplace=True``` parameter would do (Except the get_dummies function doesn't accept an ```inplace=``` parameter). (from Slack exchange on 1/4/2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df=pd.get_dummies(cat_df, columns=['EDUCATION'], prefix=['edu'])\n",
    "\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Transformations on Original df\n",
    "To avoid issues arising from having to merge 2 df, I am going to create a copy of the original df and transform the original non-numerical variables, instead of adding new variables to the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of the dataframe to retain transformations\n",
    "num_df= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to change values in SEX into numbers\n",
    "num_df['SEX']=le.fit_transform(num_df['SEX'])\n",
    "\n",
    "num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to change values in DEFUALT into numbers\n",
    "num_df['DEFAULT']=le.fit_transform(num_df['DEFAULT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use get.dummies function to transform EDU var into 4 dummy vars (1 var for each category)\n",
    "num_df=pd.get_dummies(num_df, columns=['EDUCATION'], prefix=['edu'])\n",
    "\n",
    "num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use get.dummies function to transform MAR var into 4 dummy vars (1 var for each category)\n",
    "num_df=pd.get_dummies(num_df, columns=['MARRIAGE'], prefix=['mar'])\n",
    "\"\"\"\n",
    "The values in the MARRIAGE var were numerical to start. I had to transform it into 4 dummy variables \n",
    "because I am going to use this data to train and test a regression ML model, and categorical vars are\n",
    "treated as ordinal (i.e. the algo would treat/interpret category labeled 2 as the average of categories labeled 1 & 3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Numerical df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df.to_csv (r'C:\\Users\\kpiat\\Data Science Stuff\\Course 2-Data Program\\num_only_credit_one_data.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert DEFUALT into numerical feature\n",
    "df['DEFAULT'] =df['DEFAULT'].astype('category').cat.codes\n",
    "\"\"\"\n",
    "I read that it is less memory intensive to convert non-numerical variables into numerical vars if the vars are \n",
    "have the category data type instead of the obj dtype.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain correlation co-efficients for all features and bill_ap\n",
    "df[df.columns[1:]].corr()['bill_ap'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter df to exclude cases where bill_ap value is 0, then print last 20 cases in df\n",
    "df.loc[df.bill_ap !=0].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the df to return only the rows where the april bill var is not 0\n",
    "nzbill=df.loc[df['bill_ap'] !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data to return only rows where june bill var is greater than 0\n",
    "nzbill2=df['bill_ju'].loc[lambda x: x > 0]\n",
    "\n",
    "nzbill2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter data to return only rows where the april AND june bill vars are not zero\n",
    "nzbill3=df.query('bill_ap !=0 & bill_ju !=0')\n",
    "nzbill3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Coding Notes*\n",
    ">* You can filter data using the ```df.query()``` function with string statements composed of var names and boolean operators.\n",
    ">* The first time I ran ```nzbill3.shape()``` it didn't work because I ended it with (). ```.shape``` is not a function that you want to perform on the data. It's more like asking for some kind of description of the data. That's why it shouldn't have parentheses at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the corr between the filtered data and DEFAULT\n",
    "nzbill3[nzbill3.columns[1:]].corr()['DEFAULT'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Notes*\n",
    ">* Because I am concerned that the high percentage of 0 values in the billing variables is negatively impacting the analysis, I wanted to see if the correlations with DEFAULT would improve if I excluded the data where one or more of the billing vars has a value of 0. \n",
    ">* However, it occured to me after getting the above .corr and thinking about the math behind correlation that excluding the cases with 0 values wouldn't impact the co-efficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the data to return only rows where the April bill var is less than 100,000\n",
    "df.loc[df['bill_ap']<= -100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nzbill3.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course1-env",
   "language": "python",
   "name": "course1-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
